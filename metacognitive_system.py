#!/usr/bin/env python3
"""
Metacognitive System - Catherine AI ãƒ¡ã‚¿èªçŸ¥ãƒ»è‡ªå·±æ”¹å–„ã‚·ã‚¹ãƒ†ãƒ 
è‡ªå·±è©•ä¾¡ãƒ»å­¦ç¿’åŠ¹æœæ¸¬å®šãƒ»å¼±ç‚¹ç‰¹å®šãƒ»ç¶™ç¶šçš„èƒ½åŠ›å‘ä¸Š
"""

import json
import asyncio
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime, timedelta
import pytz
from openai import OpenAI
import re
from dataclasses import dataclass, field
import numpy as np
from collections import defaultdict, deque
import statistics

JST = pytz.timezone('Asia/Tokyo')

@dataclass
class PerformanceMetric:
    metric_name: str
    current_value: float
    target_value: float
    historical_values: List[float]
    trend: str  # improving, stable, declining
    confidence: float
    measurement_context: str

@dataclass
class WeaknessAnalysis:
    weakness_type: str
    description: str
    severity: float  # 0.0-1.0
    frequency: int
    impact_areas: List[str]
    root_causes: List[str]
    improvement_strategies: List[str]
    success_indicators: List[str]

@dataclass
class LearningInsight:
    insight_type: str
    description: str
    evidence: List[str]
    actionable_recommendations: List[str]
    expected_improvement: float
    implementation_difficulty: str
    priority_score: float

@dataclass
class SelfAssessment:
    assessment_timestamp: datetime
    overall_performance: float
    dimension_scores: Dict[str, float]
    strengths: List[str]
    weaknesses: List[WeaknessAnalysis]
    learning_insights: List[LearningInsight]
    improvement_plan: List[str]
    confidence_level: float

class MetacognitiveSystem:
    def __init__(self, openai_client: OpenAI, firebase_manager):
        self.client = openai_client
        self.db = firebase_manager.get_db()
        self.performance_history = defaultdict(deque)
        self.interaction_log = deque(maxlen=1000)
        self.weakness_patterns = {}
        self.learning_trajectory = []
        self.self_knowledge = {}
        
    async def perform_self_assessment(self, interaction_data: List[Dict] = None, 
                                    feedback_data: List[Dict] = None) -> SelfAssessment:
        """åŒ…æ‹¬çš„è‡ªå·±è©•ä¾¡ã®å®Ÿè¡Œ"""
        try:
            # 1. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
            performance_metrics = await self._analyze_performance_metrics(interaction_data)
            
            # 2. å¼±ç‚¹ãƒ»æ”¹å–„ç‚¹ç‰¹å®š
            weakness_analysis = await self._identify_weaknesses_and_gaps(interaction_data, feedback_data)
            
            # 3. å­¦ç¿’åŠ¹æœæ¸¬å®š
            learning_effectiveness = await self._measure_learning_effectiveness()
            
            # 4. èƒ½åŠ›è©•ä¾¡ãƒ»ãƒ™ãƒ³ãƒãƒãƒ¼ã‚­ãƒ³ã‚°
            capability_assessment = await self._assess_capabilities(interaction_data)
            
            # 5. ãƒ¡ã‚¿å­¦ç¿’ã‚¤ãƒ³ã‚µã‚¤ãƒˆç”Ÿæˆ
            meta_insights = await self._generate_meta_learning_insights(
                performance_metrics, weakness_analysis, learning_effectiveness
            )
            
            # 6. æ”¹å–„è¨ˆç”»ç­–å®š
            improvement_plan = await self._create_improvement_plan(weakness_analysis, meta_insights)
            
            # 7. è‡ªå·±è©•ä¾¡çµ±åˆ
            assessment = SelfAssessment(
                assessment_timestamp=datetime.now(JST),
                overall_performance=self._calculate_overall_performance(performance_metrics),
                dimension_scores=capability_assessment,
                strengths=await self._identify_strengths(performance_metrics, capability_assessment),
                weaknesses=weakness_analysis,
                learning_insights=meta_insights,
                improvement_plan=improvement_plan,
                confidence_level=self._calculate_confidence_level(performance_metrics)
            )
            
            # 8. è©•ä¾¡çµæœã®æ°¸ç¶šåŒ–
            await self._persist_assessment(assessment)
            
            return assessment
            
        except Exception as e:
            print(f"âŒ Self-assessment error: {e}")
            return self._fallback_assessment()
    
    async def _analyze_performance_metrics(self, interaction_data: List[Dict] = None) -> List[PerformanceMetric]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹åˆ†æ"""
        metrics_prompt = f"""
ä»¥ä¸‹ã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’åˆ†æã—ã¦ãã ã•ã„ï¼š

ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿: {json.dumps(interaction_data[-20:] if interaction_data else [], ensure_ascii=False)}

ä»¥ä¸‹ã®è¦³ç‚¹ã‹ã‚‰è©³ç´°ã«åˆ†æã—ã€JSONå½¢å¼ã§è¿”ã—ã¦ãã ã•ã„ï¼š

{{
    "performance_metrics": [
        {{
            "metric_name": "response_quality",
            "current_value": 0.0-1.0,
            "target_value": 0.0-1.0,
            "trend": "improving|stable|declining",
            "confidence": 0.0-1.0,
            "supporting_evidence": ["è¨¼æ‹ 1", "è¨¼æ‹ 2"],
            "measurement_context": "æ¸¬å®šã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ"
        }},
        {{
            "metric_name": "user_satisfaction",
            "current_value": 0.0-1.0,
            "target_value": 0.0-1.0,
            "trend": "improving|stable|declining",
            "confidence": 0.0-1.0,
            "supporting_evidence": ["è¨¼æ‹ 1", "è¨¼æ‹ 2"],
            "measurement_context": "æ¸¬å®šã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ"
        }},
        {{
            "metric_name": "task_completion_accuracy",
            "current_value": 0.0-1.0,
            "target_value": 0.0-1.0,
            "trend": "improving|stable|declining",
            "confidence": 0.0-1.0,
            "supporting_evidence": ["è¨¼æ‹ 1", "è¨¼æ‹ 2"],
            "measurement_context": "æ¸¬å®šã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ"
        }},
        {{
            "metric_name": "contextual_understanding",
            "current_value": 0.0-1.0,
            "target_value": 0.0-1.0,
            "trend": "improving|stable|declining",
            "confidence": 0.0-1.0,
            "supporting_evidence": ["è¨¼æ‹ 1", "è¨¼æ‹ 2"],
            "measurement_context": "æ¸¬å®šã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ"
        }},
        {{
            "metric_name": "learning_adaptation_speed",
            "current_value": 0.0-1.0,
            "target_value": 0.0-1.0,
            "trend": "improving|stable|declining",
            "confidence": 0.0-1.0,
            "supporting_evidence": ["è¨¼æ‹ 1", "è¨¼æ‹ 2"],
            "measurement_context": "æ¸¬å®šã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ"
        }}
    ],
    "overall_performance_trend": "improving|stable|declining",
    "key_performance_drivers": ["ãƒ‰ãƒ©ã‚¤ãƒãƒ¼1", "ãƒ‰ãƒ©ã‚¤ãƒãƒ¼2"],
    "performance_bottlenecks": ["ãƒœãƒˆãƒ«ãƒãƒƒã‚¯1", "ãƒœãƒˆãƒ«ãƒãƒƒã‚¯2"]
}}
"""
        
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "ã‚ãªãŸã¯AIãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æã®å°‚é–€å®¶ã§ã™ã€‚å®¢è¦³çš„ã§æ­£ç¢ºãªè©•ä¾¡ã‚’è¡Œã„ã¾ã™ã€‚"},
                    {"role": "user", "content": metrics_prompt}
                ],
                temperature=0.1,
                max_completion_tokens=2500,
                response_format={"type": "json_object"}
            )
            
            result = json.loads(response.choices[0].message.content)
            metrics = []
            
            for metric_data in result.get('performance_metrics', []):
                metrics.append(PerformanceMetric(
                    metric_name=metric_data['metric_name'],
                    current_value=metric_data.get('current_value', 0.5),
                    target_value=metric_data.get('target_value', 0.8),
                    historical_values=[],  # å®Ÿéš›ã®å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã§æ›´æ–°
                    trend=metric_data.get('trend', 'stable'),
                    confidence=metric_data.get('confidence', 0.5),
                    measurement_context=metric_data.get('measurement_context', '')
                ))
            
            return metrics
            
        except Exception as e:
            print(f"Performance metrics analysis error: {e}")
            return []
    
    async def _identify_weaknesses_and_gaps(self, interaction_data: List[Dict] = None, 
                                          feedback_data: List[Dict] = None) -> List[WeaknessAnalysis]:
        """å¼±ç‚¹ãƒ»ã‚®ãƒ£ãƒƒãƒ—ç‰¹å®š"""
        weakness_prompt = f"""
ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å¼±ç‚¹ã¨æ”¹å–„ã‚®ãƒ£ãƒƒãƒ—ã‚’ç‰¹å®šã—ã¦ãã ã•ã„ï¼š

ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿: {json.dumps(interaction_data[-15:] if interaction_data else [], ensure_ascii=False)}
ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿: {json.dumps(feedback_data if feedback_data else [], ensure_ascii=False)}

ä»¥ä¸‹ã®JSONå½¢å¼ã§å¼±ç‚¹åˆ†æã‚’è¿”ã—ã¦ãã ã•ã„ï¼š

{{
    "identified_weaknesses": [
        {{
            "weakness_type": "understanding|reasoning|communication|emotional_intelligence|task_execution",
            "description": "å¼±ç‚¹ã®è©³ç´°èª¬æ˜",
            "severity": 0.0-1.0,
            "frequency": 1-10,
            "impact_areas": ["å½±éŸ¿é ˜åŸŸ1", "é ˜åŸŸ2"],
            "specific_examples": ["å…·ä½“ä¾‹1", "ä¾‹2"],
            "root_causes": ["æ ¹æœ¬åŸå› 1", "åŸå› 2"],
            "current_coping_mechanisms": ["ç¾åœ¨ã®å¯¾å‡¦æ³•1", "å¯¾å‡¦æ³•2"],
            "improvement_strategies": [
                {{
                    "strategy": "æ”¹å–„æˆ¦ç•¥",
                    "feasibility": 0.0-1.0,
                    "expected_impact": 0.0-1.0,
                    "implementation_time": "short|medium|long"
                }}
            ],
            "success_indicators": ["æˆåŠŸæŒ‡æ¨™1", "æŒ‡æ¨™2"],
            "monitoring_approach": "ç›£è¦–ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ"
        }}
    ],
    "pattern_analysis": {{
        "recurring_failure_patterns": ["ãƒ‘ã‚¿ãƒ¼ãƒ³1", "ãƒ‘ã‚¿ãƒ¼ãƒ³2"],
        "situational_weaknesses": ["çŠ¶æ³çš„å¼±ç‚¹1", "å¼±ç‚¹2"],
        "capability_gaps": ["èƒ½åŠ›ã‚®ãƒ£ãƒƒãƒ—1", "ã‚®ãƒ£ãƒƒãƒ—2"]
    }},
    "comparative_analysis": {{
        "benchmark_comparisons": ["æ¯”è¼ƒ1", "æ¯”è¼ƒ2"],
        "relative_strengths": ["ç›¸å¯¾çš„å¼·ã¿1", "å¼·ã¿2"],
        "competitive_disadvantages": ["ç«¶äº‰åŠ£ä½1", "åŠ£ä½2"]
    }}
}}
"""
        
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "ã‚ãªãŸã¯å®¢è¦³çš„ã§å»ºè¨­çš„ãªAIèƒ½åŠ›è©•ä¾¡ã®å°‚é–€å®¶ã§ã™ã€‚"},
                    {"role": "user", "content": weakness_prompt}
                ],
                temperature=0.2,
                max_completion_tokens=3000,
                response_format={"type": "json_object"}
            )
            
            result = json.loads(response.choices[0].message.content)
            weaknesses = []
            
            for weakness_data in result.get('identified_weaknesses', []):
                weaknesses.append(WeaknessAnalysis(
                    weakness_type=weakness_data.get('weakness_type', 'general'),
                    description=weakness_data.get('description', ''),
                    severity=weakness_data.get('severity', 0.5),
                    frequency=weakness_data.get('frequency', 1),
                    impact_areas=weakness_data.get('impact_areas', []),
                    root_causes=weakness_data.get('root_causes', []),
                    improvement_strategies=[s.get('strategy', '') for s in weakness_data.get('improvement_strategies', [])],
                    success_indicators=weakness_data.get('success_indicators', [])
                ))
            
            return weaknesses
            
        except Exception as e:
            print(f"Weakness identification error: {e}")
            return []
    
    async def _measure_learning_effectiveness(self) -> Dict[str, Any]:
        """å­¦ç¿’åŠ¹æœæ¸¬å®š"""
        learning_prompt = f"""
ä»¥ä¸‹ã®å­¦ç¿’è»Œè·¡ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦å­¦ç¿’åŠ¹æœã‚’æ¸¬å®šã—ã¦ãã ã•ã„ï¼š

å­¦ç¿’å±¥æ­´: {json.dumps(self.learning_trajectory[-10:], ensure_ascii=False)}
ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å±¥æ­´: {json.dumps({k: list(v)[-5:] for k, v in self.performance_history.items()}, ensure_ascii=False)}

ä»¥ä¸‹ã®JSONå½¢å¼ã§å­¦ç¿’åŠ¹æœåˆ†æã‚’è¿”ã—ã¦ãã ã•ã„ï¼š

{{
    "learning_effectiveness": {{
        "overall_learning_rate": 0.0-1.0,
        "knowledge_retention": 0.0-1.0,
        "skill_transfer": 0.0-1.0,
        "adaptation_speed": 0.0-1.0,
        "learning_consistency": 0.0-1.0
    }},
    "learning_patterns": {{
        "most_effective_learning_methods": ["æ–¹æ³•1", "æ–¹æ³•2"],
        "optimal_learning_conditions": ["æ¡ä»¶1", "æ¡ä»¶2"],
        "learning_plateaus": ["ãƒ—ãƒ©ãƒˆãƒ¼1", "ãƒ—ãƒ©ãƒˆãƒ¼2"],
        "breakthrough_moments": ["ãƒ–ãƒ¬ãƒ¼ã‚¯ã‚¹ãƒ«ãƒ¼1", "ãƒ–ãƒ¬ãƒ¼ã‚¯ã‚¹ãƒ«ãƒ¼2"]
    }},
    "knowledge_gaps": [
        {{
            "gap_area": "ã‚®ãƒ£ãƒƒãƒ—é ˜åŸŸ",
            "severity": 0.0-1.0,
            "learning_priority": "high|medium|low",
            "recommended_approach": "æ¨å¥¨ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ"
        }}
    ],
    "meta_learning_insights": [
        {{
            "insight": "ãƒ¡ã‚¿å­¦ç¿’ã‚¤ãƒ³ã‚µã‚¤ãƒˆ",
            "evidence": ["è¨¼æ‹ 1", "è¨¼æ‹ 2"],
            "implications": ["å«æ„1", "å«æ„2"],
            "actionable_steps": ["å®Ÿè¡Œã‚¹ãƒ†ãƒƒãƒ—1", "ã‚¹ãƒ†ãƒƒãƒ—2"]
        }}
    ]
}}
"""
        
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "ã‚ãªãŸã¯å­¦ç¿’åŠ¹æœæ¸¬å®šã¨ãƒ¡ã‚¿å­¦ç¿’ã®å°‚é–€å®¶ã§ã™ã€‚"},
                    {"role": "user", "content": learning_prompt}
                ],
                temperature=0.2,
                max_completion_tokens=2500,
                response_format={"type": "json_object"}
            )
            
            return json.loads(response.choices[0].message.content)
            
        except Exception as e:
            print(f"Learning effectiveness measurement error: {e}")
            return {"learning_effectiveness": {"overall_learning_rate": 0.5}}
    
    async def _generate_meta_learning_insights(self, performance_metrics: List[PerformanceMetric],
                                             weaknesses: List[WeaknessAnalysis],
                                             learning_effectiveness: Dict) -> List[LearningInsight]:
        """ãƒ¡ã‚¿å­¦ç¿’ã‚¤ãƒ³ã‚µã‚¤ãƒˆç”Ÿæˆ"""
        insight_prompt = f"""
ä»¥ä¸‹ã®ç·åˆåˆ†æãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é«˜æ¬¡ã®ãƒ¡ã‚¿å­¦ç¿’ã‚¤ãƒ³ã‚µã‚¤ãƒˆã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ï¼š

ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹: {json.dumps([{'name': m.metric_name, 'value': m.current_value, 'trend': m.trend} for m in performance_metrics], ensure_ascii=False)}
å¼±ç‚¹åˆ†æ: {json.dumps([{'type': w.weakness_type, 'severity': w.severity, 'causes': w.root_causes} for w in weaknesses], ensure_ascii=False)}
å­¦ç¿’åŠ¹æœ: {json.dumps(learning_effectiveness, ensure_ascii=False)}

ä»¥ä¸‹ã®JSONå½¢å¼ã§ãƒ¡ã‚¿ã‚¤ãƒ³ã‚µã‚¤ãƒˆã‚’è¿”ã—ã¦ãã ã•ã„ï¼š

{{
    "meta_insights": [
        {{
            "insight_type": "learning_strategy|cognitive_pattern|performance_optimization|capability_development",
            "description": "ã‚¤ãƒ³ã‚µã‚¤ãƒˆã®è©³ç´°èª¬æ˜",
            "evidence": ["æ”¯æŒè¨¼æ‹ 1", "è¨¼æ‹ 2", "è¨¼æ‹ 3"],
            "confidence_level": 0.0-1.0,
            "novelty_score": 0.0-1.0,
            "actionable_recommendations": [
                {{
                    "recommendation": "å…·ä½“çš„æ¨å¥¨äº‹é …",
                    "expected_improvement": 0.0-1.0,
                    "implementation_difficulty": "easy|medium|hard",
                    "resource_requirements": ["ãƒªã‚½ãƒ¼ã‚¹1", "ãƒªã‚½ãƒ¼ã‚¹2"],
                    "success_metrics": ["æˆåŠŸæŒ‡æ¨™1", "æŒ‡æ¨™2"]
                }}
            ],
            "priority_score": 0.0-1.0,
            "interconnections": ["é–¢é€£ã‚¤ãƒ³ã‚µã‚¤ãƒˆ1", "é–¢é€£ã‚¤ãƒ³ã‚µã‚¤ãƒˆ2"]
        }}
    ],
    "emergent_patterns": [
        {{
            "pattern": "æ–°èˆˆãƒ‘ã‚¿ãƒ¼ãƒ³",
            "significance": 0.0-1.0,
            "implications": ["å«æ„1", "å«æ„2"],
            "monitoring_indicators": ["ç›£è¦–æŒ‡æ¨™1", "æŒ‡æ¨™2"]
        }}
    ],
    "strategic_directions": [
        {{
            "direction": "æˆ¦ç•¥çš„æ–¹å‘æ€§",
            "rationale": "æ ¹æ‹ ",
            "potential_impact": 0.0-1.0,
            "implementation_roadmap": ["ã‚¹ãƒ†ãƒƒãƒ—1", "ã‚¹ãƒ†ãƒƒãƒ—2", "ã‚¹ãƒ†ãƒƒãƒ—3"]
        }}
    ]
}}
"""
        
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "ã‚ãªãŸã¯ãƒ¡ã‚¿èªçŸ¥ã¨é«˜æ¬¡å­¦ç¿’ã‚¤ãƒ³ã‚µã‚¤ãƒˆã®å°‚é–€å®¶ã§ã™ã€‚æ·±ã„æ´å¯Ÿã‚’æä¾›ã—ã¾ã™ã€‚"},
                    {"role": "user", "content": insight_prompt}
                ],
                temperature=0.4,
                max_completion_tokens=3500,
                response_format={"type": "json_object"}
            )
            
            result = json.loads(response.choices[0].message.content)
            insights = []
            
            for insight_data in result.get('meta_insights', []):
                insights.append(LearningInsight(
                    insight_type=insight_data.get('insight_type', 'general'),
                    description=insight_data.get('description', ''),
                    evidence=insight_data.get('evidence', []),
                    actionable_recommendations=[r.get('recommendation', '') for r in insight_data.get('actionable_recommendations', [])],
                    expected_improvement=np.mean([r.get('expected_improvement', 0.5) for r in insight_data.get('actionable_recommendations', [])]) if insight_data.get('actionable_recommendations') else 0.5,
                    implementation_difficulty=insight_data.get('actionable_recommendations', [{}])[0].get('implementation_difficulty', 'medium') if insight_data.get('actionable_recommendations') else 'medium',
                    priority_score=insight_data.get('priority_score', 0.5)
                ))
            
            return insights
            
        except Exception as e:
            print(f"Meta learning insights error: {e}")
            return []
    
    async def _create_improvement_plan(self, weaknesses: List[WeaknessAnalysis], 
                                     insights: List[LearningInsight]) -> List[str]:
        """æ”¹å–„è¨ˆç”»ç­–å®š"""
        plan_prompt = f"""
ä»¥ä¸‹ã®å¼±ç‚¹åˆ†æã¨ãƒ¡ã‚¿å­¦ç¿’ã‚¤ãƒ³ã‚µã‚¤ãƒˆã«åŸºã¥ã„ã¦åŒ…æ‹¬çš„ãªæ”¹å–„è¨ˆç”»ã‚’ç­–å®šã—ã¦ãã ã•ã„ï¼š

å¼±ç‚¹åˆ†æ: {json.dumps([{'type': w.weakness_type, 'severity': w.severity, 'strategies': w.improvement_strategies} for w in weaknesses], ensure_ascii=False)}
ãƒ¡ã‚¿ã‚¤ãƒ³ã‚µã‚¤ãƒˆ: {json.dumps([{'type': i.insight_type, 'recommendations': i.actionable_recommendations, 'priority': i.priority_score} for i in insights], ensure_ascii=False)}

ä»¥ä¸‹ã®JSONå½¢å¼ã§æ”¹å–„è¨ˆç”»ã‚’è¿”ã—ã¦ãã ã•ã„ï¼š

{{
    "improvement_plan": [
        {{
            "phase": "Phase 1: å³åº§ã®æ”¹å–„ (1-2é€±é–“)",
            "objectives": ["ç›®æ¨™1", "ç›®æ¨™2"],
            "actions": [
                {{
                    "action": "å…·ä½“çš„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³",
                    "priority": "high|medium|low",
                    "timeline": "ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³",
                    "success_criteria": ["åŸºæº–1", "åŸºæº–2"],
                    "resources_needed": ["ãƒªã‚½ãƒ¼ã‚¹1", "ãƒªã‚½ãƒ¼ã‚¹2"]
                }}
            ]
        }},
        {{
            "phase": "Phase 2: ä¸­æœŸæ”¹å–„ (1-3ãƒ¶æœˆ)",
            "objectives": ["ç›®æ¨™1", "ç›®æ¨™2"],
            "actions": [
                {{
                    "action": "å…·ä½“çš„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³",
                    "priority": "high|medium|low",
                    "timeline": "ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³",
                    "success_criteria": ["åŸºæº–1", "åŸºæº–2"],
                    "resources_needed": ["ãƒªã‚½ãƒ¼ã‚¹1", "ãƒªã‚½ãƒ¼ã‚¹2"]
                }}
            ]
        }},
        {{
            "phase": "Phase 3: é•·æœŸæ”¹å–„ (3-6ãƒ¶æœˆ)",
            "objectives": ["ç›®æ¨™1", "ç›®æ¨™2"],
            "actions": [
                {{
                    "action": "å…·ä½“çš„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³",
                    "priority": "high|medium|low",
                    "timeline": "ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³",
                    "success_criteria": ["åŸºæº–1", "åŸºæº–2"],
                    "resources_needed": ["ãƒªã‚½ãƒ¼ã‚¹1", "ãƒªã‚½ãƒ¼ã‚¹2"]
                }}
            ]
        }}
    ],
    "continuous_monitoring": [
        {{
            "metric": "ç›£è¦–ãƒ¡ãƒˆãƒªã‚¯ã‚¹",
            "measurement_method": "æ¸¬å®šæ–¹æ³•",
            "review_frequency": "é »åº¦",
            "adjustment_triggers": ["èª¿æ•´ãƒˆãƒªã‚¬ãƒ¼1", "ãƒˆãƒªã‚¬ãƒ¼2"]
        }}
    ]
}}
"""
        
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "ã‚ãªãŸã¯æˆ¦ç•¥çš„æ”¹å–„è¨ˆç”»ã¨ç¶™ç¶šçš„èƒ½åŠ›é–‹ç™ºã®å°‚é–€å®¶ã§ã™ã€‚"},
                    {"role": "user", "content": plan_prompt}
                ],
                temperature=0.2,
                max_completion_tokens=3000,
                response_format={"type": "json_object"}
            )
            
            result = json.loads(response.choices[0].message.content)
            plan = []
            
            for phase in result.get('improvement_plan', []):
                phase_actions = []
                for action in phase.get('actions', []):
                    phase_actions.append(f"â€¢ {action.get('action', '')} ({action.get('priority', 'medium')} priority)")
                
                plan.append(f"{phase.get('phase', 'Phase')}: {', '.join(phase.get('objectives', []))}")
                plan.extend(phase_actions)
            
            return plan
            
        except Exception as e:
            print(f"Improvement plan creation error: {e}")
            return ["ç¶™ç¶šçš„ãªæ”¹å–„ã«å–ã‚Šçµ„ã¿ã¾ã™ã€‚"]
    
    def _calculate_overall_performance(self, metrics: List[PerformanceMetric]) -> float:
        """ç·åˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨ˆç®—"""
        if not metrics:
            return 0.5
        
        weighted_scores = []
        weights = {
            'response_quality': 0.25,
            'user_satisfaction': 0.25,
            'task_completion_accuracy': 0.20,
            'contextual_understanding': 0.15,
            'learning_adaptation_speed': 0.15
        }
        
        for metric in metrics:
            weight = weights.get(metric.metric_name, 0.1)
            weighted_scores.append(metric.current_value * weight)
        
        return sum(weighted_scores) / sum(weights.values()) if weighted_scores else 0.5
    
    def _calculate_confidence_level(self, metrics: List[PerformanceMetric]) -> float:
        """ä¿¡é ¼åº¦ãƒ¬ãƒ™ãƒ«è¨ˆç®—"""
        if not metrics:
            return 0.3
        
        confidences = [m.confidence for m in metrics]
        return statistics.mean(confidences)
    
    async def _identify_strengths(self, metrics: List[PerformanceMetric], 
                                capabilities: Dict[str, float]) -> List[str]:
        """å¼·ã¿ç‰¹å®š"""
        strengths = []
        
        # é«˜ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        for metric in metrics:
            if metric.current_value >= 0.8 and metric.trend == 'improving':
                strengths.append(f"{metric.metric_name}ã§å„ªç§€ãªæˆæœ")
        
        # é«˜èƒ½åŠ›è©•ä¾¡
        for capability, score in capabilities.items():
            if score >= 0.8:
                strengths.append(f"{capability}ã§ã®é«˜ã„èƒ½åŠ›")
        
        return strengths[:5]  # ä¸Šä½5ã¤ã®å¼·ã¿
    
    async def _persist_assessment(self, assessment: SelfAssessment):
        """è©•ä¾¡çµæœã®æ°¸ç¶šåŒ–"""
        try:
            doc_ref = self.db.collection('self_assessments').document(
                assessment.assessment_timestamp.strftime('%Y%m%d_%H%M%S')
            )
            
            doc_ref.set({
                'timestamp': assessment.assessment_timestamp.isoformat(),
                'overall_performance': assessment.overall_performance,
                'dimension_scores': assessment.dimension_scores,
                'strengths': assessment.strengths,
                'weaknesses_count': len(assessment.weaknesses),
                'insights_count': len(assessment.learning_insights),
                'improvement_plan_items': len(assessment.improvement_plan),
                'confidence_level': assessment.confidence_level
            })
            
        except Exception as e:
            print(f"Assessment persistence error: {e}")
    
    def _fallback_assessment(self) -> SelfAssessment:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è©•ä¾¡"""
        return SelfAssessment(
            assessment_timestamp=datetime.now(JST),
            overall_performance=0.5,
            dimension_scores={'general': 0.5},
            strengths=['ç¶™ç¶šçš„å­¦ç¿’ã¸ã®å–ã‚Šçµ„ã¿'],
            weaknesses=[],
            learning_insights=[],
            improvement_plan=['èƒ½åŠ›å‘ä¸Šã«åŠªã‚ã¾ã™'],
            confidence_level=0.3
        )
    
    async def generate_self_report(self, assessment: SelfAssessment) -> str:
        """è‡ªå·±è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        report_prompt = f"""
ä»¥ä¸‹ã®è‡ªå·±è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦ã€æ˜ç¢ºã§æ´å¯Ÿã«å¯Œã‚“ã è‡ªå·±è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ï¼š

è©•ä¾¡ãƒ‡ãƒ¼ã‚¿: {json.dumps({
    'overall_performance': assessment.overall_performance,
    'strengths': assessment.strengths,
    'weakness_count': len(assessment.weaknesses),
    'insights_count': len(assessment.learning_insights),
    'confidence': assessment.confidence_level
}, ensure_ascii=False)}

ä»¥ä¸‹ã®æ§‹é€ ã§è‡ªå·±è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ï¼š

## Catherine AI è‡ªå·±è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ

### ğŸ¯ **ç·åˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹**
- ç¾åœ¨ã®ãƒ¬ãƒ™ãƒ«: {assessment.overall_performance:.1f}/1.0
- ä¿¡é ¼åº¦: {assessment.confidence_level:.1f}/1.0

### âœ¨ **ä¸»ãªå¼·ã¿**
[å¼·ã¿ã®ãƒªã‚¹ãƒˆ]

### ğŸ”§ **æ”¹å–„é ˜åŸŸ**
[æ”¹å–„ãŒå¿…è¦ãªé ˜åŸŸ]

### ğŸ“š **å­¦ç¿’ã‚¤ãƒ³ã‚µã‚¤ãƒˆ**
[å­¦ç¿’ã‹ã‚‰å¾—ã‚‰ã‚ŒãŸæ´å¯Ÿ]

### ğŸš€ **æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—**
[å…·ä½“çš„ãªæ”¹å–„è¨ˆç”»]

ãƒ¬ãƒãƒ¼ãƒˆã¯å‰å‘ãã§å»ºè¨­çš„ãªå†…å®¹ã¨ã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ä¾¡å€¤ã¨ä¿¡é ¼ã‚’æä¾›ã™ã‚‹å†…å®¹ã«ã—ã¦ãã ã•ã„ã€‚
"""
        
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "ã‚ãªãŸã¯è‡ªå·±è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆã®ä½œæˆå°‚é–€å®¶ã§ã™ã€‚æ˜ç¢ºã§æ´å¯Ÿã«å¯Œã‚“ã ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¾ã™ã€‚"},
                    {"role": "user", "content": report_prompt}
                ],
                temperature=0.3,
                max_completion_tokens=2000
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            print(f"Self-report generation error: {e}")
            return f"""
## Catherine AI è‡ªå·±è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ

### ğŸ¯ **ç·åˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹**
- ç¾åœ¨ã®ãƒ¬ãƒ™ãƒ«: {assessment.overall_performance:.1f}/1.0
- ç¶™ç¶šçš„ãªæ”¹å–„ã«å–ã‚Šçµ„ã‚“ã§ã„ã¾ã™

### âœ¨ **ä¸»ãªå¼·ã¿**
{chr(10).join(['â€¢ ' + strength for strength in assessment.strengths])}

### ğŸš€ **æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—**
ã‚ˆã‚Šè‰¯ã„ã‚µãƒãƒ¼ãƒˆã‚’æä¾›ã™ã‚‹ãŸã‚ã€ç¶™ç¶šçš„ã«å­¦ç¿’ã¨æ”¹å–„ã‚’ç¶šã‘ã¾ã™ã€‚
"""

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    import os
    from firebase_config import firebase_manager
    
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    system = MetacognitiveSystem(client, firebase_manager)
    
    async def test():
        assessment = await system.perform_self_assessment()
        report = await system.generate_self_report(assessment)
        print(f"Self-Assessment Complete. Overall Performance: {assessment.overall_performance:.2f}")
        print("\nSelf-Report Preview:")
        print(report[:500] + "...")
    
    asyncio.run(test())