import asyncio
import discord
import speech_recognition as sr
import io
import wave
from typing import Optional, Callable
from openai import OpenAI
import tempfile
import os

class VoiceManager:
    def __init__(self, openai_client: OpenAI):
        self.openai_client = openai_client
        self.recognizer = sr.Recognizer()
        self.is_recording = False
        self.voice_client: Optional[discord.VoiceClient] = None
        self.audio_buffer = []
        
    async def join_voice_channel(self, voice_channel: discord.VoiceChannel) -> bool:
        """ãƒœã‚¤ã‚¹ãƒãƒ£ãƒ³ãƒãƒ«ã«å‚åŠ """
        try:
            self.voice_client = await voice_channel.connect()
            print(f"âœ… Joined voice channel: {voice_channel.name}")
            return True
        except Exception as e:
            print(f"âŒ Failed to join voice channel: {e}")
            return False
    
    async def leave_voice_channel(self) -> bool:
        """ãƒœã‚¤ã‚¹ãƒãƒ£ãƒ³ãƒãƒ«ã‹ã‚‰é€€å‡º"""
        try:
            if self.voice_client:
                await self.voice_client.disconnect()
                self.voice_client = None
                print("âœ… Left voice channel")
                return True
            return False
        except Exception as e:
            print(f"âŒ Failed to leave voice channel: {e}")
            return False
    
    async def start_listening(self, callback: Callable[[str], None]) -> bool:
        """éŸ³å£°èªè­˜é–‹å§‹"""
        if not self.voice_client:
            return False
        
        try:
            self.is_recording = True
            
            # ã‚«ã‚¹ã‚¿ãƒ éŸ³å£°ã‚·ãƒ³ã‚¯ã‚’ä½œæˆ
            voice_sink = VoiceSink(self.recognizer, self.openai_client, callback)
            self.voice_client.start_recording(voice_sink, self._recording_finished)
            
            print("ğŸ¤ Started voice recognition")
            return True
            
        except Exception as e:
            print(f"âŒ Failed to start voice recognition: {e}")
            return False
    
    async def stop_listening(self) -> bool:
        """éŸ³å£°èªè­˜åœæ­¢"""
        try:
            if self.voice_client and self.is_recording:
                self.voice_client.stop_recording()
                self.is_recording = False
                print("ğŸ”‡ Stopped voice recognition")
                return True
            return False
        except Exception as e:
            print(f"âŒ Failed to stop voice recognition: {e}")
            return False
    
    def _recording_finished(self, sink, channel, *args):
        """éŒ²éŸ³å®Œäº†æ™‚ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        print("ğŸ“¼ Recording finished")

class VoiceSink(discord.sinks.WaveSink):
    """ã‚«ã‚¹ã‚¿ãƒ éŸ³å£°ã‚·ãƒ³ã‚¯ - éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†"""
    
    def __init__(self, recognizer: sr.Recognizer, openai_client: OpenAI, 
                 text_callback: Callable[[str], None]):
        super().__init__()
        self.recognizer = recognizer
        self.openai_client = openai_client
        self.text_callback = text_callback
        self.silence_threshold = 1.0  # ç§’
        self.last_audio_time = 0
    
    def wants_opus(self) -> bool:
        return False
    
    def write(self, data, user):
        """éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã®æ›¸ãè¾¼ã¿å‡¦ç†"""
        if user:  # ãƒœãƒƒãƒˆã®éŸ³å£°ã¯é™¤å¤–
            super().write(data, user)
            
            # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ãŒä¸€å®šé‡æºœã¾ã£ãŸã‚‰å‡¦ç†
            if len(self.audio_data.get(user.id, [])) > 48000:  # ç´„1ç§’åˆ†
                asyncio.create_task(self._process_audio(user.id))
    
    async def _process_audio(self, user_id: int):
        """éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›"""
        try:
            if user_id not in self.audio_data:
                return
            
            # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’ WAV å½¢å¼ã§ä¿å­˜
            with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_file:
                with wave.open(temp_file.name, 'wb') as wav_file:
                    wav_file.setnchannels(2)  # ã‚¹ãƒ†ãƒ¬ã‚ª
                    wav_file.setsampwidth(2)  # 16bit
                    wav_file.setframerate(48000)  # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å‘¨æ³¢æ•°
                    
                    # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’æ›¸ãè¾¼ã¿
                    audio_bytes = b''.join(self.audio_data[user_id])
                    wav_file.writeframes(audio_bytes)
                
                # OpenAI Whisperã§éŸ³å£°èªè­˜
                text = await self._transcribe_with_whisper(temp_file.name)
                
                if text and text.strip():
                    self.text_callback(f"ğŸ¤ {text}")
                
                # å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªã‚¢
                self.audio_data[user_id] = []
                
                # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
                os.unlink(temp_file.name)
                
        except Exception as e:
            print(f"âŒ Audio processing error: {e}")
    
    async def _transcribe_with_whisper(self, audio_file_path: str) -> Optional[str]:
        """OpenAI Whisper APIã§éŸ³å£°ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›"""
        try:
            with open(audio_file_path, 'rb') as audio_file:
                transcript = self.openai_client.audio.transcriptions.create(
                    model="whisper-1",
                    file=audio_file,
                    language="ja"  # æ—¥æœ¬èªæŒ‡å®š
                )
                return transcript.text
                
        except Exception as e:
            print(f"âŒ Whisper transcription error: {e}")
            return None

class VoiceCommands:
    """éŸ³å£°ã‚³ãƒãƒ³ãƒ‰å‡¦ç†ã‚¯ãƒ©ã‚¹"""
    
    @staticmethod
    async def process_voice_command(text: str, voice_manager: VoiceManager) -> str:
        """éŸ³å£°ã‹ã‚‰èªè­˜ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’å‡¦ç†"""
        text_lower = text.lower()
        
        # éŸ³å£°ã‚³ãƒãƒ³ãƒ‰ã®åˆ¤å®š
        if "ã˜ã‚‡ã„ã‚“" in text_lower or "å‚åŠ " in text_lower:
            return "voice_join"
        elif "ã‚Šãƒ¼ã¶" in text_lower or "é€€å‡º" in text_lower or "ãƒã‚¤ãƒã‚¤" in text_lower:
            return "voice_leave"
        elif "ã¨ã…ãƒ¼ã©ã…ãƒ¼" in text_lower or "ã‚„ã‚‹ã“ã¨" in text_lower or "ã‚¿ã‚¹ã‚¯" in text_lower:
            return "todo_create"
        elif "ã‚Šã™ã¨" in text_lower or "ä¸€è¦§" in text_lower:
            return "todo_list"
        elif "å®Œäº†" in text_lower or "çµ‚ã‚ã£ãŸ" in text_lower:
            return "todo_done"
        else:
            return "conversation"  # é€šå¸¸ã®ä¼šè©±ã¨ã—ã¦å‡¦ç†
    
    @staticmethod
    def extract_todo_from_voice(text: str) -> Optional[str]:
        """éŸ³å£°ã‹ã‚‰ToDoã‚’æŠ½å‡º"""
        # ã€Œã€œã‚’ã‚„ã‚‹ã€ã€Œã€œã™ã‚‹ã€ã€Œã€œã‚’è¿½åŠ ã€ãªã©ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º
        todo_patterns = [
            "ã‚’ã‚„ã‚‹", "ã™ã‚‹", "ã‚’è¿½åŠ ", "ã‚’ã—ã¦", "ã‚’ã‚„ã£ã¦",
            "ã‚’ãŠé¡˜ã„", "ã‚’å¿˜ã‚Œãšã«", "ã‚’ãƒªãƒã‚¤ãƒ³ãƒ‰"
        ]
        
        for pattern in todo_patterns:
            if pattern in text:
                # ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å‰ã®éƒ¨åˆ†ã‚’ToDoã¨ã—ã¦æŠ½å‡º
                todo_text = text.split(pattern)[0].strip()
                if len(todo_text) > 2:  # æœ€ä½é™ã®é•·ã•ãƒã‚§ãƒƒã‚¯
                    return todo_text
        
        return None
