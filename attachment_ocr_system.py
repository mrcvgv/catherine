#!/usr/bin/env python3
"""
Attachment OCR to ToDo System
æ·»ä»˜â†’OCRâ†’è¦ç´„â†’ToDoåŒ–ï¼šç”»åƒ/PDF/æ–‡æ›¸ã‹ã‚‰è‡ªå‹•ã‚¿ã‚¹ã‚¯æŠ½å‡º
"""

import base64
import io
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
import pytz
from openai import OpenAI
from firebase_config import firebase_manager
import re

JST = pytz.timezone('Asia/Tokyo')

@dataclass
class ExtractedTask:
    title: str
    description: str
    priority: int  # 1-5
    category: str
    estimated_hours: float
    assignee: Optional[str] = None
    due_date: Optional[datetime] = None
    confidence: float = 0.5
    source_context: str = ""

@dataclass
class DocumentAnalysis:
    document_type: str  # "invoice", "estimate", "meeting_notes", "email", "specification", "other"
    key_information: Dict[str, Any]
    extracted_tasks: List[ExtractedTask]
    summary: str
    next_actions: List[str]
    urgency_level: int  # 1-5
    filing_suggestion: str

class AttachmentOCRSystem:
    def __init__(self, openai_client: OpenAI):
        self.openai_client = openai_client
        self.db = firebase_manager.get_db()
        
        # ã‚µãƒãƒ¼ãƒˆã™ã‚‹æ–‡æ›¸ã‚¿ã‚¤ãƒ—
        self.document_patterns = {
            'invoice': r'(è«‹æ±‚æ›¸|invoice|bill|payment)',
            'estimate': r'(è¦‹ç©|estimate|quotation|proposal)',
            'meeting_notes': r'(è­°äº‹éŒ²|minutes|meeting|ä¼šè­°|æ‰“åˆã›)',
            'email': r'(from:|to:|subject:|ä»¶å|å®›å…ˆ)',
            'specification': r'(ä»•æ§˜æ›¸|specification|requirements|è¦ä»¶)',
            'contract': r'(å¥‘ç´„|contract|agreement|åˆæ„)',
            'report': r'(å ±å‘Šæ›¸|report|ãƒ¬ãƒãƒ¼ãƒˆ|analysis)',
            'manual': r'(ãƒãƒ‹ãƒ¥ã‚¢ãƒ«|manual|æ‰‹é †|procedure)'
        }
    
    async def process_attachment(self, user_id: str, attachment_data: bytes, 
                               filename: str, attachment_type: str = "image") -> DocumentAnalysis:
        """æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¦ã‚¿ã‚¹ã‚¯ã‚’æŠ½å‡º"""
        try:
            # 1) OCR/ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
            extracted_text = await self._extract_text(attachment_data, attachment_type)
            
            if not extracted_text or len(extracted_text) < 10:
                return DocumentAnalysis(
                    document_type="unknown",
                    key_information={},
                    extracted_tasks=[],
                    summary="ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ",
                    next_actions=[],
                    urgency_level=1,
                    filing_suggestion="manual_review"
                )
            
            # 2) æ–‡æ›¸ã‚¿ã‚¤ãƒ—åˆ¤å®š
            doc_type = self._classify_document_type(extracted_text, filename)
            
            # 3) AIåˆ†æã§ã‚¿ã‚¹ã‚¯æŠ½å‡º
            analysis = await self._analyze_document_content(
                extracted_text, doc_type, filename, user_id
            )
            
            return analysis
            
        except Exception as e:
            print(f"âŒ Attachment processing error: {e}")
            return self._get_fallback_analysis()
    
    async def _extract_text(self, attachment_data: bytes, 
                           attachment_type: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºï¼ˆOCR/èª­ã¿å–ã‚Šï¼‰"""
        try:
            if attachment_type == "image":
                return await self._extract_text_from_image(attachment_data)
            elif attachment_type == "pdf":
                return await self._extract_text_from_pdf(attachment_data)
            elif attachment_type == "text":
                return attachment_data.decode('utf-8')
            else:
                # ãã®ä»–ã®å½¢å¼ã‚‚GPT-4 Visionã§è©¦è¡Œ
                return await self._extract_text_from_image(attachment_data)
                
        except Exception as e:
            print(f"âŒ Text extraction error: {e}")
            return ""
    
    async def _extract_text_from_image(self, image_data: bytes) -> str:
        """ç”»åƒã‹ã‚‰OCRã§ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º"""
        try:
            # Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
            base64_image = base64.b64encode(image_data).decode('utf-8')
            
            # GPT-4 Visionã§ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
            response = self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": "ã“ã®ç”»åƒã‹ã‚‰ã™ã¹ã¦ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æ­£ç¢ºã«æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚‚å¯èƒ½ãªé™ã‚Šä¿æŒã—ã¦ãã ã•ã„ã€‚"
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{base64_image}"
                                }
                            }
                        ]
                    }
                ],
                max_tokens=2000,
                temperature=0.1
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            print(f"âŒ Image OCR error: {e}")
            return ""
    
    async def _extract_text_from_pdf(self, pdf_data: bytes) -> str:
        """PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º"""
        try:
            # ç°¡æ˜“å®Ÿè£…ï¼šPDFâ†’ç”»åƒâ†’OCRã®æµã‚Œ
            # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ PyPDF2 ã‚„ pdfplumber ã‚’ä½¿ç”¨
            return "PDFèª­ã¿å–ã‚Šæ©Ÿèƒ½ã¯é–‹ç™ºä¸­ã§ã™"
            
        except Exception as e:
            print(f"âŒ PDF extraction error: {e}")
            return ""
    
    def _classify_document_type(self, text: str, filename: str) -> str:
        """æ–‡æ›¸ã‚¿ã‚¤ãƒ—ã‚’åˆ†é¡"""
        text_lower = text.lower()
        filename_lower = filename.lower()
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰åˆ¤å®š
        for doc_type, pattern in self.document_patterns.items():
            if re.search(pattern, filename_lower):
                return doc_type
        
        # å†…å®¹ã‹ã‚‰åˆ¤å®š
        for doc_type, pattern in self.document_patterns.items():
            if re.search(pattern, text_lower):
                return doc_type
        
        return "other"
    
    async def _analyze_document_content(self, text: str, doc_type: str, 
                                       filename: str, user_id: str) -> DocumentAnalysis:
        """æ–‡æ›¸å†…å®¹ã‚’AIåˆ†æ"""
        try:
            prompt = f"""
ä»¥ä¸‹ã®æ–‡æ›¸ã‚’åˆ†æã—ã€å®Ÿè¡Œå¯èƒ½ãªã‚¿ã‚¹ã‚¯ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ï¼š

ã€ãƒ•ã‚¡ã‚¤ãƒ«åã€‘{filename}
ã€æ–‡æ›¸ã‚¿ã‚¤ãƒ—ã€‘{doc_type}
ã€å†…å®¹ã€‘
{text[:2000]}

ä»¥ä¸‹ã®JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ï¼š
{{
    "document_type": "åˆ†é¡ã•ã‚ŒãŸæ–‡æ›¸ã‚¿ã‚¤ãƒ—",
    "summary": "æ–‡æ›¸ã®è¦ç´„ï¼ˆ100å­—ä»¥å†…ï¼‰",
    "key_information": {{
        "deadline": "æœŸé™ãŒã‚ã‚Œã°æŠ½å‡ºï¼ˆYYYY-MM-DDå½¢å¼ï¼‰",
        "amount": "é‡‘é¡ãŒã‚ã‚Œã°æŠ½å‡º",
        "contact_person": "é€£çµ¡å…ˆãŒã‚ã‚Œã°æŠ½å‡º",
        "project_name": "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåãŒã‚ã‚Œã°æŠ½å‡º"
    }},
    "extracted_tasks": [
        {{
            "title": "ã‚¿ã‚¹ã‚¯åï¼ˆ30å­—ä»¥å†…ï¼‰",
            "description": "è©³ç´°èª¬æ˜ï¼ˆ100å­—ä»¥å†…ï¼‰",
            "priority": 1-5ã®å„ªå…ˆåº¦,
            "category": "work/personal/communication/finance/other",
            "estimated_hours": æ¨å®šä½œæ¥­æ™‚é–“,
            "assignee": "æ‹…å½“è€…ï¼ˆæ˜è¨˜ã•ã‚Œã¦ã„ã‚Œã°ï¼‰",
            "due_date": "æœŸé™ï¼ˆYYYY-MM-DDå½¢å¼ã€æ¨å®šå¯èƒ½ãªã‚‰ï¼‰",
            "confidence": 0.0-1.0ã®ç¢ºä¿¡åº¦
        }}
    ],
    "next_actions": [
        "ã™ãã«å®Ÿè¡Œã™ã¹ãè¡Œå‹•1",
        "ã™ãã«å®Ÿè¡Œã™ã¹ãè¡Œå‹•2"
    ],
    "urgency_level": 1-5ã®ç·Šæ€¥åº¦,
    "filing_suggestion": "ã©ã“ã«ä¿å­˜ã™ã¹ãã‹ã®ææ¡ˆ"
}}
"""
            
            response = self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "system", "content": prompt}],
                temperature=0.3,
                response_format={"type": "json_object"}
            )
            
            import json
            result = json.loads(response.choices[0].message.content)
            
            # ExtractedTaskã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
            extracted_tasks = []
            for task_data in result.get('extracted_tasks', []):
                due_date = None
                if task_data.get('due_date'):
                    try:
                        due_date = datetime.fromisoformat(task_data['due_date']).replace(tzinfo=JST)
                    except:
                        pass
                
                task = ExtractedTask(
                    title=task_data['title'],
                    description=task_data['description'],
                    priority=task_data['priority'],
                    category=task_data['category'],
                    estimated_hours=task_data['estimated_hours'],
                    assignee=task_data.get('assignee'),
                    due_date=due_date,
                    confidence=task_data.get('confidence', 0.5),
                    source_context=f"æ–‡æ›¸: {filename}"
                )
                extracted_tasks.append(task)
            
            return DocumentAnalysis(
                document_type=result['document_type'],
                key_information=result['key_information'],
                extracted_tasks=extracted_tasks,
                summary=result['summary'],
                next_actions=result['next_actions'],
                urgency_level=result['urgency_level'],
                filing_suggestion=result['filing_suggestion']
            )
            
        except Exception as e:
            print(f"âŒ Document analysis error: {e}")
            return self._get_fallback_analysis()
    
    def _get_fallback_analysis(self) -> DocumentAnalysis:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åˆ†æçµæœ"""
        return DocumentAnalysis(
            document_type="unknown",
            key_information={},
            extracted_tasks=[],
            summary="æ–‡æ›¸ã®è‡ªå‹•åˆ†æã«å¤±æ•—ã—ã¾ã—ãŸ",
            next_actions=["æ‰‹å‹•ã§å†…å®¹ã‚’ç¢ºèª"],
            urgency_level=2,
            filing_suggestion="manual_review"
        )
    
    async def create_todos_from_analysis(self, analysis: DocumentAnalysis, 
                                       user_id: str, team_mode: bool = True) -> List[str]:
        """åˆ†æçµæœã‹ã‚‰ToDoã‚’ä½œæˆ"""
        try:
            created_todo_ids = []
            
            for task in analysis.extracted_tasks:
                if task.confidence < 0.3:  # ç¢ºä¿¡åº¦ãŒä½ã„ã‚‚ã®ã¯ã‚¹ã‚­ãƒƒãƒ—
                    continue
                
                if team_mode:
                    # ãƒãƒ¼ãƒ ToDoä½œæˆ
                    from team_todo_manager import TeamTodoManager
                    team_manager = TeamTodoManager(self.openai_client)
                    
                    todo_data = await team_manager.create_team_todo(
                        creator_id=user_id,
                        title=task.title,
                        description=f"{task.description}\n\nğŸ“ {task.source_context}",
                        assignee=task.assignee or "unassigned",
                        category=task.category,
                        priority=task.priority,
                        due_date=task.due_date,
                        tags=['auto_extracted', analysis.document_type]
                    )
                    
                    if todo_data:
                        created_todo_ids.append(todo_data['todo_id'])
                
                else:
                    # å€‹äººToDoä½œæˆï¼ˆå®Ÿè£…ã¯ç’°å¢ƒã«å¿œã˜ã¦ï¼‰
                    pass
            
            return created_todo_ids
            
        except Exception as e:
            print(f"âŒ Todo creation from analysis error: {e}")
            return []
    
    def format_analysis_report(self, analysis: DocumentAnalysis, 
                              created_todo_ids: List[str] = None) -> str:
        """åˆ†æçµæœãƒ¬ãƒãƒ¼ãƒˆã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        urgency_emoji = {1: "ğŸ“", 2: "ğŸ“‹", 3: "âš¡", 4: "ğŸ”¥", 5: "ğŸš¨"}
        
        report = f"ğŸ“„ **æ–‡æ›¸åˆ†æãƒ¬ãƒãƒ¼ãƒˆ** {urgency_emoji.get(analysis.urgency_level, 'ğŸ“')}\n\n"
        
        # æ–‡æ›¸ã‚¿ã‚¤ãƒ—ã¨è¦ç´„
        report += f"**ğŸ“‚ ç¨®åˆ¥**: {analysis.document_type}\n"
        report += f"**ğŸ“ è¦ç´„**: {analysis.summary}\n\n"
        
        # é‡è¦æƒ…å ±
        if analysis.key_information:
            report += f"**ğŸ” é‡è¦æƒ…å ±**:\n"
            for key, value in analysis.key_information.items():
                if value:
                    key_jp = {
                        'deadline': 'æœŸé™',
                        'amount': 'é‡‘é¡',
                        'contact_person': 'é€£çµ¡å…ˆ',
                        'project_name': 'ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ'
                    }.get(key, key)
                    report += f"â€¢ {key_jp}: {value}\n"
            report += "\n"
        
        # æŠ½å‡ºã•ã‚ŒãŸã‚¿ã‚¹ã‚¯
        if analysis.extracted_tasks:
            report += f"**âš¡ æŠ½å‡ºã•ã‚ŒãŸã‚¿ã‚¹ã‚¯** ({len(analysis.extracted_tasks)}ä»¶):\n"
            for i, task in enumerate(analysis.extracted_tasks, 1):
                confidence_emoji = "ğŸŸ¢" if task.confidence >= 0.8 else "ğŸŸ¡" if task.confidence >= 0.5 else "ğŸŸ "
                due_text = f" (æœŸé™: {task.due_date.strftime('%m/%d')})" if task.due_date else ""
                report += f"{i}. {confidence_emoji} **{task.title}** ({task.estimated_hours}h){due_text}\n"
                report += f"   ğŸ“ {task.description}\n"
            report += "\n"
        
        # ä½œæˆã•ã‚ŒãŸToDo
        if created_todo_ids:
            report += f"âœ… **ä½œæˆã•ã‚ŒãŸToDo**: {len(created_todo_ids)}ä»¶\n\n"
        
        # æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
        if analysis.next_actions:
            report += f"**â¡ï¸ æ¨å¥¨ã•ã‚Œã‚‹æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³**:\n"
            for action in analysis.next_actions:
                report += f"â€¢ {action}\n"
            report += "\n"
        
        # ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ææ¡ˆ
        if analysis.filing_suggestion and analysis.filing_suggestion != "manual_review":
            report += f"**ğŸ“ ä¿å­˜å…ˆææ¡ˆ**: {analysis.filing_suggestion}\n"
        
        return report
    
    async def batch_process_attachments(self, user_id: str, 
                                       attachments: List[Dict]) -> str:
        """è¤‡æ•°æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€æ‹¬å‡¦ç†"""
        try:
            all_analyses = []
            all_created_todos = []
            
            for attachment in attachments:
                analysis = await self.process_attachment(
                    user_id,
                    attachment['data'],
                    attachment['filename'],
                    attachment.get('type', 'image')
                )
                all_analyses.append((attachment['filename'], analysis))
                
                # ToDoä½œæˆ
                created_todos = await self.create_todos_from_analysis(analysis, user_id)
                all_created_todos.extend(created_todos)
            
            # ãƒãƒƒãƒãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            batch_report = f"ğŸ“ **ä¸€æ‹¬æ–‡æ›¸å‡¦ç†ãƒ¬ãƒãƒ¼ãƒˆ** ({len(attachments)}ä»¶)\n\n"
            
            total_tasks = sum(len(analysis.extracted_tasks) for _, analysis in all_analyses)
            batch_report += f"ğŸ“Š **ã‚µãƒãƒªãƒ¼**:\n"
            batch_report += f"â€¢ å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«: {len(attachments)}ä»¶\n"
            batch_report += f"â€¢ æŠ½å‡ºã‚¿ã‚¹ã‚¯: {total_tasks}ä»¶\n"
            batch_report += f"â€¢ ä½œæˆToDo: {len(all_created_todos)}ä»¶\n\n"
            
            # å„ãƒ•ã‚¡ã‚¤ãƒ«ã®æ¦‚è¦
            batch_report += f"**ğŸ“„ å‡¦ç†çµæœ**:\n"
            for filename, analysis in all_analyses:
                urgency_emoji = {1: "ğŸ“", 2: "ğŸ“‹", 3: "âš¡", 4: "ğŸ”¥", 5: "ğŸš¨"}
                batch_report += f"â€¢ {filename} {urgency_emoji.get(analysis.urgency_level, 'ğŸ“')}: "
                batch_report += f"{len(analysis.extracted_tasks)}ä»¶ã®ã‚¿ã‚¹ã‚¯ã‚’æŠ½å‡º\n"
            
            return batch_report
            
        except Exception as e:
            print(f"âŒ Batch processing error: {e}")
            return "ä¸€æ‹¬å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"